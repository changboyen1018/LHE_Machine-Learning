{"cells":[{"cell_type":"markdown","metadata":{"id":"HhIgGq3za0yh"},"source":["\n","# HW6 Diffusion Model\n","\n","**Sources:**\n","- Github implementation [Denoising Diffusion Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n","- Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)\n"]},{"cell_type":"markdown","metadata":{"id":"wLHSIArLcFK0"},"source":["## Import Packages and Set Seeds"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-30T06:17:55.374214Z","iopub.status.busy":"2023-03-30T06:17:55.373297Z","iopub.status.idle":"2023-03-30T06:18:43.218774Z","shell.execute_reply":"2023-03-30T06:18:43.217348Z","shell.execute_reply.started":"2023-03-30T06:17:55.374161Z"},"id":"s1xegyILIuLz","outputId":"b71ce929-c0bd-4ff0-ff53-cb01b421c2e9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: einops in ./.local/lib/python3.10/site-packages (0.6.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.28.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.13.4)\n","Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.24.2)\n","Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2023.3.23)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: ema_pytorch in ./.local/lib/python3.10/site-packages (0.2.3)\n","Requirement already satisfied: torch>=1.6 in ./.local/lib/python3.10/site-packages (from ema_pytorch) (1.13.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.10/site-packages (from torch>=1.6->ema_pytorch) (11.7.99)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.10/site-packages (from torch>=1.6->ema_pytorch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.10/site-packages (from torch>=1.6->ema_pytorch) (11.7.99)\n","Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from torch>=1.6->ema_pytorch) (4.5.0)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.10/site-packages (from torch>=1.6->ema_pytorch) (8.5.0.96)\n","Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->ema_pytorch) (0.37.1)\n","Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->ema_pytorch) (59.6.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (0.18.0)\n","Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n","Requirement already satisfied: torch>=1.4.0 in ./.local/lib/python3.10/site-packages (from accelerate) (1.13.1)\n","Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from accelerate) (23.0)\n","Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from accelerate) (5.9.4)\n","Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from accelerate) (1.24.2)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n","Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (4.5.0)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n","Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.37.1)\n","Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (59.6.0)\n"]}],"source":["!pip install einops\n","!pip install transformers\n","!pip install ema_pytorch\n","!pip install accelerate"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"LQnlc27k7Aiw","outputId":"122e675a-a91b-4fa3-d56f-1f6bb38bc094"},"outputs":[],"source":["import math\n","import copy\n","from pathlib import Path\n","from random import random\n","from functools import partial\n","from collections import namedtuple\n","from multiprocessing import cpu_count\n","\n","import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from torch.optim import Adam\n","\n","import torchvision\n","from torchvision import transforms as T, utils\n","\n","from einops import rearrange, reduce, repeat\n","from einops.layers.torch import Rearrange\n","\n","from PIL import Image\n","from tqdm.auto import tqdm\n","from ema_pytorch import EMA\n","\n","from accelerate import Accelerator\n","import matplotlib.pyplot as plt\n","import os\n","\n","torch.backends.cudnn.benchmark = True\n","torch.manual_seed(4096)\n","\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed(4096)"]},{"cell_type":"markdown","metadata":{"id":"Rj17psVw7Shg"},"source":["## Step 1: Forward process (Noise scheduler)\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"qWw50ui9IZ5q"},"outputs":[],"source":["def linear_beta_schedule(timesteps):\n","    \"\"\"\n","    linear schedule, proposed in original ddpm paper\n","    \"\"\"\n","    scale = 1000 / timesteps\n","    beta_start = scale * 0.0001\n","    beta_end = scale * 0.02\n","    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n","\n","def extract(a, t, x_shape):\n","    b, *_ = t.shape\n","    out = a.gather(-1, t)\n","    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"]},{"cell_type":"markdown","metadata":{"id":"Vt6JSKawk7_b"},"source":["Create dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uuckjpW_k1LN"},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(\n","        self,\n","        folder,\n","        image_size\n","    ):\n","        self.folder = folder\n","        self.image_size = image_size\n","        self.paths = [p for p in Path(f'{folder}').glob(f'**/*.jpg')]\n","        #################################\n","        ## TODO: Data Augmentation ##\n","        #################################\n","        self.transform = T.Compose([\n","            T.Resize(image_size),\n","            T.ToTensor()\n","        ])\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, index):\n","        path = self.paths[index]\n","        img = Image.open(path)\n","        return self.transform(img)"]},{"cell_type":"markdown","metadata":{"id":"buW6BaNga-XH"},"source":["## Step 2: The backward process = U-Net\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iYw6u0nJXIWy"},"source":["Define some useful functions and U-Net"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"DuJCCZ5dInQq"},"outputs":[],"source":["def exists(x):\n","    return x is not None\n","\n","def default(val, d):\n","    if exists(val):\n","        return val\n","    return d() if callable(d) else d\n","\n","def identity(t, *args, **kwargs):\n","    return t\n","\n","def cycle(dl):\n","    while True:\n","        for data in dl:\n","            yield data\n","\n","def has_int_squareroot(num):\n","    return (math.sqrt(num) ** 2) == num\n","\n","def num_to_groups(num, divisor):\n","    groups = num // divisor\n","    remainder = num % divisor\n","    arr = [divisor] * groups\n","    if remainder > 0:\n","        arr.append(remainder)\n","    return arr\n","\n","# normalization functions\n","\n","def normalize_to_neg_one_to_one(img):\n","    return img * 2 - 1\n","\n","def unnormalize_to_zero_to_one(t):\n","    return (t + 1) * 0.5\n","\n","# small helper modules\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","\n","    def forward(self, x, *args, **kwargs):\n","        return self.fn(x, *args, **kwargs) + x\n","\n","def Upsample(dim, dim_out = None):\n","    return nn.Sequential(\n","        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n","        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n","    )\n","\n","def Downsample(dim, dim_out = None):\n","    return nn.Sequential(\n","        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n","        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n","    )\n","\n","class WeightStandardizedConv2d(nn.Conv2d):\n","    \"\"\"\n","    https://arxiv.org/abs/1903.10520\n","    weight standardization purportedly works synergistically with group normalization\n","    \"\"\"\n","    def forward(self, x):\n","        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n","\n","        weight = self.weight\n","        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n","        var = reduce(weight, 'o ... -> o 1 1 1', partial(torch.var, unbiased = False))\n","        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n","\n","        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n","\n","    def forward(self, x):\n","        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n","        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n","        mean = torch.mean(x, dim = 1, keepdim = True)\n","        return (x - mean) * (var + eps).rsqrt() * self.g\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.fn = fn\n","        self.norm = LayerNorm(dim)\n","\n","    def forward(self, x):\n","        x = self.norm(x)\n","        return self.fn(x)\n","\n","# sinusoidal positional embeds\n","\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n","    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n","    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n","\n","    def __init__(self, dim, is_random = False):\n","        super().__init__()\n","        assert (dim % 2) == 0\n","        half_dim = dim // 2\n","        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n","\n","    def forward(self, x):\n","        x = rearrange(x, 'b -> b 1')\n","        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n","        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n","        fouriered = torch.cat((x, fouriered), dim = -1)\n","        return fouriered\n","\n","# building block modules\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, dim_out, groups = 8):\n","        super().__init__()\n","        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding = 1)\n","        self.norm = nn.GroupNorm(groups, dim_out)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x, scale_shift = None):\n","        x = self.proj(x)\n","        x = self.norm(x)\n","\n","        if exists(scale_shift):\n","            scale, shift = scale_shift\n","            x = x * (scale + 1) + shift\n","\n","        x = self.act(x)\n","        return x\n","\n","class ResnetBlock(nn.Module):\n","    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n","        super().__init__()\n","        self.mlp = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(time_emb_dim, dim_out * 2)\n","        ) if exists(time_emb_dim) else None\n","\n","        self.block1 = Block(dim, dim_out, groups = groups)\n","        self.block2 = Block(dim_out, dim_out, groups = groups)\n","        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n","\n","    def forward(self, x, time_emb = None):\n","\n","        scale_shift = None\n","        if exists(self.mlp) and exists(time_emb):\n","            time_emb = self.mlp(time_emb)\n","            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n","            scale_shift = time_emb.chunk(2, dim = 1)\n","\n","        h = self.block1(x, scale_shift = scale_shift)\n","\n","        h = self.block2(h)\n","\n","        return h + self.res_conv(x)\n","\n","class LinearAttention(nn.Module):\n","    def __init__(self, dim, heads = 4, dim_head = 32):\n","        super().__init__()\n","        self.scale = dim_head ** -0.5\n","        self.heads = heads\n","        hidden_dim = dim_head * heads\n","        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Conv2d(hidden_dim, dim, 1),\n","            LayerNorm(dim)\n","        )\n","\n","    def forward(self, x):\n","        b, c, h, w = x.shape\n","        qkv = self.to_qkv(x).chunk(3, dim = 1)\n","        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n","\n","        q = q.softmax(dim = -2)\n","        k = k.softmax(dim = -1)\n","\n","        q = q * self.scale\n","        v = v / (h * w)\n","\n","        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n","\n","        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n","        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n","        return self.to_out(out)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 4, dim_head = 32):\n","        super().__init__()\n","        self.scale = dim_head ** -0.5\n","        self.heads = heads\n","        hidden_dim = dim_head * heads\n","\n","        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n","        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n","\n","    def forward(self, x):\n","        b, c, h, w = x.shape\n","        qkv = self.to_qkv(x).chunk(3, dim = 1)\n","        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n","\n","        q = q * self.scale\n","\n","        sim = torch.einsum('b h d i, b h d j -> b h i j', q, k)\n","        attn = sim.softmax(dim = -1)\n","        out = torch.einsum('b h i j, b h d j -> b h i d', attn, v)\n","\n","        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n","        return self.to_out(out)\n","\n","# model\n","\n","class Unet(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        init_dim = None,\n","        out_dim = None,\n","        dim_mults=(1, 2, 4, 8),\n","        channels = 3,\n","        resnet_block_groups = 8,\n","        learned_sinusoidal_cond = False,\n","        random_fourier_features = False,\n","        learned_sinusoidal_dim = 16\n","    ):\n","        super().__init__()\n","\n","        # determine dimensions\n","\n","        self.channels = channels\n","\n","        init_dim = default(init_dim, dim)\n","        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)\n","\n","        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n","        in_out = list(zip(dims[:-1], dims[1:]))\n","\n","        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n","\n","        # time embeddings\n","\n","        time_dim = dim * 4\n","\n","        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n","\n","        if self.random_or_learned_sinusoidal_cond:\n","            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n","            fourier_dim = learned_sinusoidal_dim + 1\n","        else:\n","            sinu_pos_emb = SinusoidalPosEmb(dim)\n","            fourier_dim = dim\n","\n","        self.time_mlp = nn.Sequential(\n","            sinu_pos_emb,\n","            nn.Linear(fourier_dim, time_dim),\n","            nn.GELU(),\n","            nn.Linear(time_dim, time_dim)\n","        )\n","\n","        # layers\n","\n","        self.downs = nn.ModuleList([])\n","        self.ups = nn.ModuleList([])\n","        num_resolutions = len(in_out)\n","\n","        for ind, (dim_in, dim_out) in enumerate(in_out):\n","            is_last = ind >= (num_resolutions - 1)\n","\n","            self.downs.append(nn.ModuleList([\n","                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n","                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n","                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n","                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n","            ]))\n","\n","        mid_dim = dims[-1]\n","        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n","        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n","        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n","\n","        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n","            is_last = ind == (len(in_out) - 1)\n","\n","            self.ups.append(nn.ModuleList([\n","                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n","                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n","                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n","                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n","            ]))\n","\n","        self.out_dim = default(out_dim, channels)\n","\n","        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n","        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n","\n","    def forward(self, x, time):\n","        x = self.init_conv(x)\n","        r = x.clone()\n","\n","        t = self.time_mlp(time)\n","\n","        h = []\n","\n","        for block1, block2, attn, downsample in self.downs:\n","            x = block1(x, t)\n","            h.append(x)\n","\n","            x = block2(x, t)\n","            x = attn(x)\n","            h.append(x)\n","\n","            x = downsample(x)\n","\n","        x = self.mid_block1(x, t)\n","        x = self.mid_attn(x)\n","        x = self.mid_block2(x, t)\n","\n","        for block1, block2, attn, upsample in self.ups:\n","            x = torch.cat((x, h.pop()), dim = 1)\n","            x = block1(x, t)\n","\n","            x = torch.cat((x, h.pop()), dim = 1)\n","            x = block2(x, t)\n","            x = attn(x)\n","\n","            x = upsample(x)\n","\n","        x = torch.cat((x, r), dim = 1)\n","\n","        x = self.final_res_block(x, t)\n","        return self.final_conv(x)\n","model = Unet(64)"]},{"cell_type":"markdown","metadata":{"id":"8B9GlZrotBXy"},"source":["## Step 3: The Diffusion Process\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ph05t8MxXMoY"},"source":["Define diffusion process, including generating noisy models, sample...\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"X7TKWoZpInQs"},"outputs":[],"source":["class GaussianDiffusion(nn.Module):\n","    def __init__(\n","        self,\n","        model,\n","        *,\n","        image_size,\n","        timesteps = 1000,\n","        beta_schedule = 'linear',\n","        auto_normalize = True\n","    ):\n","        super().__init__()\n","        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n","        assert not model.random_or_learned_sinusoidal_cond\n","\n","        self.model = model\n","\n","        self.channels = self.model.channels\n","\n","        self.image_size = image_size\n","\n","\n","        if beta_schedule == 'linear':\n","            beta_schedule_fn = linear_beta_schedule\n","        else:\n","            raise ValueError(f'unknown beta schedule {beta_schedule}')\n","        \n","        # calculate beta and other precalculated parameters\n","        betas = beta_schedule_fn(timesteps)\n","                                            \n","        alphas = 1. - betas\n","        alphas_cumprod = torch.cumprod(alphas, dim=0)\n","        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n","\n","        timesteps, = betas.shape\n","        self.num_timesteps = int(timesteps)\n","\n","        # sampling related parameters\n","\n","        self.sampling_timesteps = timesteps # default num sampling timesteps to number of timesteps at training\n","\n","        # helper function to register buffer from float64 to float32\n","\n","        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n","\n","        register_buffer('betas', betas)\n","        register_buffer('alphas_cumprod', alphas_cumprod)\n","        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n","\n","        # calculations for diffusion q(x_t | x_{t-1}) and others\n","\n","        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n","        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n","        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n","        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n","        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n","\n","        # calculations for posterior q(x_{t-1} | x_t, x_0)\n","\n","        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n","\n","        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n","\n","        register_buffer('posterior_variance', posterior_variance)\n","\n","        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n","\n","        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n","        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n","        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n","\n","        # derive loss weight\n","        # snr - signal noise ratio\n","\n","        snr = alphas_cumprod / (1 - alphas_cumprod)\n","\n","        # https://arxiv.org/abs/2303.09556\n","\n","        maybe_clipped_snr = snr.clone()\n","\n","        register_buffer('loss_weight', maybe_clipped_snr / snr)\n","\n","        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n","\n","        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n","        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n","\n","    def predict_start_from_noise(self, x_t, t, noise):\n","        return (\n","            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n","            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n","        )\n","\n","    def predict_noise_from_start(self, x_t, t, x0):\n","        return (\n","            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n","            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n","        )\n","\n","\n","    def q_posterior(self, x_start, x_t, t):\n","        posterior_mean = (\n","            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n","            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n","        )\n","        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n","        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n","        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n","\n","    def model_predictions(self, x, t, clip_x_start = False, rederive_pred_noise = False):\n","        model_output = self.model(x, t)\n","        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n","\n","        pred_noise = model_output\n","        x_start = self.predict_start_from_noise(x, t, pred_noise)\n","        x_start = maybe_clip(x_start)\n","\n","        if clip_x_start and rederive_pred_noise:\n","            pred_noise = self.predict_noise_from_start(x, t, x_start)\n","\n","        return pred_noise, x_start\n","\n","    def p_mean_variance(self, x, t, clip_denoised = True):\n","        noise, x_start = self.model_predictions(x, t)\n","\n","        if clip_denoised:\n","            x_start.clamp_(-1., 1.)\n","\n","        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n","        return model_mean, posterior_variance, posterior_log_variance, x_start\n","\n","    @torch.no_grad()\n","    def p_sample(self, x, t: int):\n","        b, *_, device = *x.shape, x.device\n","        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n","        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, clip_denoised = True)\n","        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n","        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n","        return pred_img, x_start\n","\n","    @torch.no_grad()\n","    def p_sample_loop(self, shape, return_all_timesteps = False):\n","        batch, device = shape[0], self.betas.device\n","\n","        img = torch.randn(shape, device = device)\n","        imgs = [img]\n","\n","        x_start = None\n","        \n","        ###########################################\n","        ## TODO: plot the sampling process ##\n","        ###########################################\n","        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n","            img, x_start = self.p_sample(img, t)\n","            imgs.append(img)\n","        \n","        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n","\n","        ret = self.unnormalize(ret)\n","        return ret\n","\n","    @torch.no_grad()\n","    def sample(self, batch_size = 16, return_all_timesteps = False):\n","        image_size, channels = self.image_size, self.channels\n","        sample_fn = self.p_sample_loop\n","        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n","\n","\n","    def q_sample(self, x_start, t, noise=None):\n","        noise = default(noise, lambda: torch.randn_like(x_start))\n","\n","        return (\n","            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n","            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n","        )\n","\n","    @property\n","    def loss_fn(self):\n","        return F.mse_loss\n","\n","\n","    def p_losses(self, x_start, t, noise = None):\n","        b, c, h, w = x_start.shape\n","        noise = default(noise, lambda: torch.randn_like(x_start))\n","\n","        # noise sample\n","\n","        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n","\n","        # predict and take gradient step\n","\n","        model_out = self.model(x, t)\n","\n","        loss = self.loss_fn(model_out, noise, reduction = 'none')\n","        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n","\n","        loss = loss * extract(self.loss_weight, t, loss.shape)\n","        return loss.mean()\n","\n","    def forward(self, img, *args, **kwargs):\n","        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n","        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n","        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n","\n","        img = self.normalize(img)\n","        return self.p_losses(img, t, *args, **kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"yWJUjFIHInQt"},"source":["Define Trainer: define the updating process"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Ed12NNXPtDon"},"outputs":[],"source":["class Trainer(object):\n","    def __init__(\n","        self,\n","        diffusion_model,\n","        folder,\n","        *,\n","        train_batch_size = 16,\n","        gradient_accumulate_every = 1,\n","        train_lr = 1e-4,\n","        train_num_steps = 100000,\n","        ema_update_every = 10,\n","        ema_decay = 0.995,\n","        adam_betas = (0.9, 0.99),\n","        save_and_sample_every = 1000,\n","        num_samples = 25,\n","        results_folder = './results',\n","        split_batches = True,\n","        inception_block_idx = 2048\n","    ):\n","        super().__init__()\n","\n","        # accelerator\n","\n","        self.accelerator = Accelerator(\n","            split_batches = split_batches,\n","            mixed_precision = 'no'\n","        )\n","        \n","\n","        # model\n","\n","        self.model = diffusion_model\n","        self.channels = diffusion_model.channels\n","\n","        # sampling and training hyperparameters\n","\n","        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n","        self.num_samples = num_samples\n","        self.save_and_sample_every = save_and_sample_every\n","\n","        self.batch_size = train_batch_size\n","        self.gradient_accumulate_every = gradient_accumulate_every\n","\n","        self.train_num_steps = train_num_steps\n","        self.image_size = diffusion_model.image_size\n","\n","        # dataset and dataloader\n","\n","        self.ds = Dataset(folder, self.image_size)\n","        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n","\n","        dl = self.accelerator.prepare(dl)\n","        self.dl = cycle(dl)\n","\n","        # optimizer\n","\n","        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n","\n","        # for logging results in a folder periodically\n","\n","        if self.accelerator.is_main_process:\n","            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n","            self.ema.to(self.device)\n","\n","        self.results_folder = Path(results_folder)\n","        self.results_folder.mkdir(exist_ok = True)\n","\n","        # step counter state\n","\n","        self.step = 0\n","\n","        # prepare model, dataloader, optimizer with accelerator\n","\n","        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n","\n","    @property\n","    def device(self):\n","        return self.accelerator.device\n","\n","    def save(self, milestone):\n","        if not self.accelerator.is_local_main_process:\n","            return\n","\n","        data = {\n","            'step': self.step,\n","            'model': self.accelerator.get_state_dict(self.model),\n","            'opt': self.opt.state_dict(),\n","            'ema': self.ema.state_dict(),\n","            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n","        }\n","\n","        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n","\n","    def load(self, ckpt):\n","        accelerator = self.accelerator\n","        device = accelerator.device\n","\n","        data = torch.load(ckpt, map_location=device)\n","\n","        model = self.accelerator.unwrap_model(self.model)\n","        model.load_state_dict(data['model'])\n","\n","        self.step = data['step']\n","        self.opt.load_state_dict(data['opt'])\n","        if self.accelerator.is_main_process:\n","            self.ema.load_state_dict(data[\"ema\"])\n","\n","\n","        if exists(self.accelerator.scaler) and exists(data['scaler']):\n","            self.accelerator.scaler.load_state_dict(data['scaler'])\n","\n","\n","    def train(self):\n","        accelerator = self.accelerator\n","        device = accelerator.device\n","\n","        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n","\n","            while self.step < self.train_num_steps:\n","\n","                total_loss = 0.\n","\n","                for _ in range(self.gradient_accumulate_every):\n","                    data = next(self.dl).to(device)\n","\n","                    with self.accelerator.autocast():\n","                        loss = self.model(data)\n","                        loss = loss / self.gradient_accumulate_every\n","                        total_loss += loss.item()\n","\n","                    self.accelerator.backward(loss)\n","\n","                accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n","                pbar.set_description(f'loss: {total_loss:.4f}')\n","\n","                accelerator.wait_for_everyone()\n","\n","                self.opt.step()\n","                self.opt.zero_grad()\n","\n","                accelerator.wait_for_everyone()\n","\n","                self.step += 1\n","                if accelerator.is_main_process:\n","                    self.ema.update()\n","\n","                    if self.step != 0 and self.step % self.save_and_sample_every == 0:\n","                        self.ema.ema_model.eval()\n","\n","                        with torch.no_grad():\n","                            milestone = self.step // self.save_and_sample_every\n","                            batches = num_to_groups(self.num_samples, self.batch_size)\n","                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n","\n","                        all_images = torch.cat(all_images_list, dim = 0)\n","\n","                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n","                        \n","                        self.save(milestone)\n","\n","                pbar.update(1)\n","\n","        accelerator.print('training complete')\n","        \n","    def inference(self, num=1000, n_iter=5, output_path='./submission'):\n","        if not os.path.exists(output_path):\n","            os.mkdir(output_path)\n","        with torch.no_grad():\n","            for i in range(n_iter):\n","                batches = num_to_groups(num // n_iter, 200)\n","                all_images = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))[0]\n","                for j in range(all_images.size(0)):\n","                    torchvision.utils.save_image(all_images[j], f'{output_path}/{i * 200 + j + 1}.jpg')              \n","                "]},{"cell_type":"markdown","metadata":{"id":"TZM7HR-UInQu"},"source":["# Training Hyper-parameters"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wOZPtVPvInQu"},"outputs":[{"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m Unet(\n\u001b[1;32m     16\u001b[0m     dim \u001b[39m=\u001b[39m channels,\n\u001b[1;32m     17\u001b[0m     dim_mults \u001b[39m=\u001b[39m dim_mults\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m diffusion \u001b[39m=\u001b[39m GaussianDiffusion(\n\u001b[1;32m     21\u001b[0m     model,\n\u001b[1;32m     22\u001b[0m     image_size \u001b[39m=\u001b[39m IMG_SIZE,\n\u001b[1;32m     23\u001b[0m     timesteps \u001b[39m=\u001b[39m timesteps,\n\u001b[1;32m     24\u001b[0m     beta_schedule \u001b[39m=\u001b[39m beta_schedule\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m     diffusion,\n\u001b[1;32m     29\u001b[0m     path,\n\u001b[1;32m     30\u001b[0m     train_batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[1;32m     31\u001b[0m     train_lr \u001b[39m=\u001b[39;49m lr,\n\u001b[1;32m     32\u001b[0m     train_num_steps \u001b[39m=\u001b[39;49m train_num_steps,\n\u001b[1;32m     33\u001b[0m     gradient_accumulate_every \u001b[39m=\u001b[39;49m grad_steps,\n\u001b[1;32m     34\u001b[0m     ema_decay \u001b[39m=\u001b[39;49m ema_decay,\n\u001b[1;32m     35\u001b[0m     save_and_sample_every \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n","Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, diffusion_model, folder, train_batch_size, gradient_accumulate_every, train_lr, train_num_steps, ema_update_every, ema_decay, adam_betas, save_and_sample_every, num_samples, results_folder, split_batches, inception_block_idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m# dataset and dataloader\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds \u001b[39m=\u001b[39m Dataset(folder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size)\n\u001b[0;32m---> 50\u001b[0m dl \u001b[39m=\u001b[39m DataLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds, batch_size \u001b[39m=\u001b[39;49m train_batch_size, shuffle \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, pin_memory \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, num_workers \u001b[39m=\u001b[39;49m cpu_count())\n\u001b[1;32m     52\u001b[0m dl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mprepare(dl)\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m cycle(dl)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:344\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 344\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["path = '/kaggle/input/diffusion/faces/faces'\n","IMG_SIZE = 64             # Size of images, do not change this if you do not know why you need to change\n","batch_size = 16\n","train_num_steps = 10000        # total training steps\n","lr = 1e-3\n","grad_steps = 1            # gradient accumulation steps, the equivalent batch size for updating equals to batch_size * grad_steps = 16 * 1\n","ema_decay = 0.995           # exponential moving average decay\n","\n","channels = 16             # Numbers of channels of the first layer of CNN\n","dim_mults = (1, 2, 4)        # The model size will be (channels, 2 * channels, 4 * channels, 4 * channels, 2 * channels, channels)\n","\n","timesteps = 100            # Number of steps (adding noise)\n","beta_schedule = 'linear'\n","\n","model = Unet(\n","    dim = channels,\n","    dim_mults = dim_mults\n",")\n","\n","diffusion = GaussianDiffusion(\n","    model,\n","    image_size = IMG_SIZE,\n","    timesteps = timesteps,\n","    beta_schedule = beta_schedule\n",")\n","\n","trainer = Trainer(\n","    diffusion,\n","    path,\n","    train_batch_size = batch_size,\n","    train_lr = lr,\n","    train_num_steps = train_num_steps,\n","    gradient_accumulate_every = grad_steps,\n","    ema_decay = ema_decay,\n","    save_and_sample_every = 1000\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"SV7OL7PvInQu"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHoY_6CrInQv","outputId":"010af6c5-a426-42cd-b560-721fff3baa84"},"outputs":[],"source":["ckpt = '/content/drive/MyDrive/ML 2023 Spring/model-55.pt'\n","trainer.load(ckpt)\n","trainer.inference()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkWpuU-2KzIL","outputId":"2215341d-4f7f-48c3-85ac-403e9ad2cb27"},"outputs":[],"source":["%cd ./submission\n","!tar -zcf ../submission.tgz *.jpg\n","%cd .."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
